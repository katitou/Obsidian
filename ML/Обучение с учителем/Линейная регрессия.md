
Задача классификации и регрессии - поиск отображения из множества объектов *X*  в множество возможных таргетов.

$$
X \to {Y}
$$

* классификация: $$ X \to {0, 1,\dots, K} $$
* регрессия: $$ X \to {R} $$
Что такое таргеты 
𝑦
y?
Таргеты 
𝑦
y обозначают классы данных, которые мы пытаемся разделить с помощью классификатора.

Если объект принадлежит положительному классу, 
𝑦
=
1
y=1.
Если объект принадлежит отрицательному классу, 
𝑦
=
−
1
y=−1.
Математически, принадлежность объекта 
𝑥
x классу определяется с помощью знака скалярного произведения:

𝑦
=
sign
(
⟨
𝑤
,
𝑥
⟩
)
,
y=sign(⟨w,x⟩),
где:

⟨
𝑤
,
𝑥
⟩
⟨w,x⟩ — скалярное произведение вектора весов 
𝑤
w и признаков объекта 
𝑥
x,
sign
sign — функция, возвращающая знак числа (
+
1
+1 для положительных чисел, 
−
1
−1 для отрицательных).
Таким образом, классификатор предсказывает класс объекта 
𝑥
x, основываясь на знаке значения 
⟨
𝑤
,
𝑥
⟩
⟨w,x⟩.

2. Что такое функционал ошибки?
Чтобы оценить, насколько хорошо работает классификатор, мы используем функционал ошибки. Он измеряет количество ошибок классификатора. Формула:

∑
𝑖
𝐼
[
𝑦
𝑖
≠
sign
(
⟨
𝑤
,
𝑥
𝑖
⟩
)
]
,
i
∑
​
 I[y 
i
​
 

=sign(⟨w,x 
i
​
 ⟩)],
где:

𝑦
𝑖
y 
i
​
  — реальный класс объекта,
sign
(
⟨
𝑤
,
𝑥
𝑖
⟩
)
sign(⟨w,x 
i
​
 ⟩) — предсказанный класс объекта,
𝐼
[
условие
]
I[условие] — индикаторная функция, которая равна 
1
1, если условие выполняется, и 
0
0 в противном случае.
Эта формула говорит: считаем все объекты, где реальный класс 
𝑦
𝑖
y 
i
​
  не совпадает с предсказанным знаком скалярного произведения.

Цель классификации: минимизировать число ошибок, т.е. минимизировать этот функционал по 
𝑤
w:

∑
𝑖
𝐼
[
𝑦
𝑖
≠
sign
(
⟨
𝑤
,
𝑥
𝑖
⟩
)
]
→
min
⁡
𝑤
.
i
∑
​
 I[y 
i
​
 

=sign(⟨w,x 
i
​
 ⟩)]→ 
w
min
​
 .
3. Упростим функционал с помощью домножения на 
𝑦
𝑖
y 
i
​
 :
Заметим, что 
𝑦
𝑖
≠
sign
(
⟨
𝑤
,
𝑥
𝑖
⟩
)
y 
i
​
 

=sign(⟨w,x 
i
​
 ⟩) эквивалентно 
𝑦
𝑖
⋅
⟨
𝑤
,
𝑥
𝑖
⟩
<
0
y 
i
​
 ⋅⟨w,x 
i
​
 ⟩<0.
Почему?

Если 
𝑦
𝑖
=
±
1
y 
i
​
 =±1, то 
𝑦
𝑖
⋅
⟨
𝑤
,
𝑥
𝑖
⟩
y 
i
​
 ⋅⟨w,x 
i
​
 ⟩ будет положительным, только если знак 
𝑦
𝑖
y 
i
​
  совпадает со знаком 
⟨
𝑤
,
𝑥
𝑖
⟩
⟨w,x 
i
​
 ⟩.
Подставляем это в функционал:

∑
𝑖
𝐼
[
𝑦
𝑖
⟨
𝑤
,
𝑥
𝑖
⟩
<
0
]
→
min
⁡
𝑤
.
i
∑
​
 I[y 
i
​
 ⟨w,x 
i
​
 ⟩<0]→ 
w
min
​
 .
Это означает: мы минимизируем количество объектов, где 
𝑦
𝑖
⟨
𝑤
,
𝑥
𝑖
⟩
<
0
y 
i
​
 ⟨w,x 
i
​
 ⟩<0 (то есть неправильные классификации).

4. Что такое отступ 
𝑀
M?
Величина:

𝑀
=
𝑦
𝑖
⋅
⟨
𝑤
,
𝑥
𝑖
⟩
M=y 
i
​
 ⋅⟨w,x 
i
​
 ⟩
называется отступом (margin) классификатора.

Если 
𝑀
>
0
M>0, объект классифицирован верно.
Чем больше 
𝑀
M, тем больше уверенность классификатора (объект далеко от разделяющей гиперплоскости).
Если 
𝑀
<
0
M<0, объект классифицирован неверно.
Чем меньше 
𝑀
M (по модулю), тем более серьезна ошибка классификатора.
5. Интерпретация misclassification loss:
Функция потерь misclassification loss:

∑
𝑖
𝐼
[
𝑀
<
0
]
i
∑
​
 I[M<0]
говорит: мы считаем все объекты, у которых отступ 
𝑀
M отрицателен. То есть мы минимизируем количество объектов, где классификатор ошибается.

6. Итог:
Отступ 
𝑀
M определяет верность и уверенность классификатора для каждого объекта.
Мы минимизируем ошибки (объекты с 
𝑀
<
0
M<0).
Чем больше 
𝑀
>
0
M>0, тем лучше классификатор "уверен" в своем решении.





