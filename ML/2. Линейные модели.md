
- [[#Линейная модель|Линейная модель]]
- [[#Линейная регрессия|Линейная регрессия]]
- [[#Точное решение|Точное решение]]
- [[#Точное решение#Псевдо-обратная матрица|Псевдо-обратная матрица]]



Задачи классификации и регрессии - это задачи поиска отображения множества объектов $X$ в множество таргетов.

- Классификация: $X -> {0, 1, ..., K}$, где $K$ - номера классов.
- Регрессия: $X -> R$.

Ограничим функции отображения (поскольку их бесконечно много), будем искать наилучшее отображение в заранее заданном параметризованном семействе функций, в случае линейных моделей - линейные функции вида:

$$y = w_1x_1 + ... + w_dx_d + w_0 $$
где
- $y$ - таргет
- $(x_1, ..., x_D)$ - вектор признаков объекта выборки (фичи - англ features)
- $w_1, ..., w_D, w_0$ - параметры модели (вектор весов)

### Линейная модель 

$$y = <x, w> + w_0$$
Теперь ищем не абстрактное отображение, а конкретный вектор  $w_1, ..., w_D, w_0 \in \mathbb {R}^{D+1}$ 


![[Pasted image 20250303123658.png]]
	    *Рис. 1: Классификация - ищем разделяющее правило.
		Регрессия - пытаемся приблизить значение $y$ какой-то прямой.

### Линейная регрессия

Задача регрессии - найти линейную комбинацию столбцов $(x_1, ..., x_D)$, которая наилучшим способом приближает столбец $y$ по евклидовой норме (расстояние, $L_2$, MSE).

Датасет $(X, y)$ - $X$ - матрица объектов-признаков, $y$ - целевая переменная.

$$f_w(x_i) = <w, x_i> + w_0$$

$$
(x_{i1}, \dots, x_{iD}) \cdot 
\begin{bmatrix} w_1 \\ \vdots \\ w_D \end{bmatrix} + w_0 =
\begin{bmatrix} 1 & x_{i1} & \dots & x_{iD} \end{bmatrix} \cdot
\begin{bmatrix} w_0 \\ w_1 \\ \vdots \\ w_D \end{bmatrix}
$$

Метрика качества (функция потерь, функционал качества, лосс) - функционал, оценивающий точность приближения функции модели. 

	- Функция - сопоставляют один объект множества объекту другого множества.
	- Функционал - сопоставляет функцию (отображение) из одного множества с числом из другого множества.

MSE:
$$
MSE(f, X, y) = 1/N \sum||y-Xw||_2^2
$$

Чем меньше функция потерь, тем точнее решение, значит, чтобы найти лучшую модель, нужно минимизировать ошибку по $w$:

$$||y-Xw||_2^2 -> \min\limits_{w}$$
:
![[Pasted image 20250303132813.png|400]]
*Рис: Функция потерь


![[Pasted image 20250303132736.png|500]]
*Рис.: Матричная запись линейной регресии




#### Аналитическое решение линейной регресии

![[Pasted image 20250303144508.png]]
![[Pasted image 20250303144543.png]]




Преимущества линейных моделей: 
- интерпретируемость (по весам модели понятно, какие признаки она учитывала, какие более важные, какие менее важные, какие вообще не играют роли в конечном результате) 
- надежные
- легко применить (при должной обработке дадут хороший результат, покрывающий 80% точности)
- не переобучаются (либо хорошо борются с переобучением с помощью регуляризации)

![[Pasted image 20250303121504.png]]



![[Pasted image 20250223162520.png]]

![[Pasted image 20250223163104.png]]


![[Pasted image 20250223164811.png]]

![[Pasted image 20250223165515.png]]
![[Pasted image 20250223165528.png]]
![[Pasted image 20250223165610.png]]



## Точное решение

![[Pasted image 20250223161411.png]]



### Псевдо-обратная матрица 

![[Pasted image 20250223162653.png]]
![[Pasted image 20250223162726.png]]
![[Pasted image 20250223162737.png]]
