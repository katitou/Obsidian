
Задачи классификации и регрессии - это задачи поиска отображения множества объектов $X$ в множество таргетов.

- Классификация: $X -> {0, 1, ..., K}$, где $K$ - номера классов.
- Регрессия: $X -> R$.

Ограничим функции отображения (поскольку их бесконечно много), будем искать наилучшее отображение в заранее заданном параметризованном семействе функций, в случае линейных моделей - линейные функции вида:

$$y = w_1x_1 + ... + w_dx_d + w_0 $$
где
- $y$ - таргет
- $(x_1, ..., x_D)$ - вектор признаков объекта выборки (фичи - англ features)
- $w_1, ..., w_D, w_0$ - параметры модели (вектор весов)

### Линейная модель 

$$y = <x, w> + w_0$$
Теперь ищем не абстрактное отображение, а конкретный вектор  $w_1, ..., w_D, w_0 $ 






Преимущества линейных моделей: 
- интерпретируемость (по весам модели понятно, какие признаки она учитывала, какие более важные, какие менее важные, какие вообще не играют роли в конечном результате) 
- надежные
- легко применить (при должной обработке дадут хороший результат, покрывающий 80% точности)
- не переобучаются (либо хорошо борются с переобучением с помощью регуляризации)

![[Pasted image 20250303121504.png]]



![[Pasted image 20250223162520.png]]

![[Pasted image 20250223163104.png]]


![[Pasted image 20250223164811.png]]

![[Pasted image 20250223165515.png]]
![[Pasted image 20250223165528.png]]
![[Pasted image 20250223165610.png]]



## Точное решение

![[Pasted image 20250223161411.png]]



### Псевдо-обратная матрица 

![[Pasted image 20250223162653.png]]
![[Pasted image 20250223162726.png]]
![[Pasted image 20250223162737.png]]
